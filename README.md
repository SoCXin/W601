# [cluster](https://github.com/tfzoo/cluster) 
[![sites](tfzoo/tfzoo.png)](http://www.tfzoo.com)
####   qitas@qitas.cn
### [简介](https://github.com/tfzoo/cluster/wiki) 

聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。

因为聚类是一种无监督学习（数据无标注），通常使用数据可视化评价结果。

### [参考](tfzoo/) 

#### [K-means](https://github.com/src-d/kmcuda) 

 K-means算法是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一。K-means算法的基本思想是：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。

* 优点： 速度快，计算简便 
* 缺点： 我们必须提前知道数据有多少类/组。

K-Medians是与K-Means类似的一种变体，它是通过计算类中所有向量的中值，而不是平均值，来确定簇的中心点。

这种方法的优点是对数据中的异常值不太敏感，但是在较大的数据集时进行聚类时，速度相对于K-Means较慢，造成这种现象的原因是这种方法每次迭代时，都需要对数据进行排序。

#### [Meanshift](https://github.com/mattnedrich/MeanShift_py) 

Mean-Shift是一种基于滑动窗口的聚类算法。也可以说它是一种基于质心的算法，这意思是它是通过计算滑动窗口中的均值来更新中心点的候选框，以此达到找到每个簇中心点的目的。然后在剩下的处理阶段中，对这些候选窗口进行滤波以消除近似或重复的窗口，找到最终的中心点及其对应的簇。

具体步骤： 
1. 确定滑动窗口半径r，以随机选取的中心点C半径为r的圆形滑动窗口开始滑动。均值漂移类似一种爬山算法，在每一次迭代中向密度更高的区域移动，直到收敛。 
2. 每一次滑动到新的区域，计算滑动窗口内的均值来作为中心点，滑动窗口内的点的数量为窗口内的密度。在每一次移动中，窗口会想密度更高的区域移动。 
3. 移动窗口，计算窗口内的中心点以及窗口内的密度，知道没有方向在窗口内可以容纳更多的点，即一直移动到圆内密度不再增加为止。 
4. 步骤一到三会产生很多个滑动窗口，当多个滑动窗口重叠时，保留包含最多点的窗口，然后根据数据点所在的滑动窗口进行聚类。 

优点：均值漂移聚类算法不需要我们知道有多少类/组；基于密度的算法相比于K-Means受均值影响较小。 

缺点：窗口半径r的选择可能是不重要的。

#### [DBSCAN](https://github.com/mhahsler/dbscan) 

DBSCAN（Density-Based Spatial Clustering of Applications with Noise,具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇(即要求聚类空间中的一定区域内所包含对象的数目不小于某一给定阈值)，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。

在DBSCAN算法中将数据点分为一下三类： 
* 核心点：在半径Eps内含有超过MinPts数目的点 
* 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内 
* 噪音点：既不是核心点也不是边界点的点 

具体步骤： 
1. 首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。 
2. 重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。 


优点：不需要知道簇的数量 

缺点：需要确定距离r和minPoints

####  [Hierarchical](https://github.com/richliao/textClassifier) 

分层聚类算法实际上分为两类：自上而下或自下而上。自下而上的算法首先将每个数据点视为一个单一的簇，然后连续地合并（或聚合）成对的簇，直到所有的簇都合并成一个包含所有数据点的簇。因此，自下而上的分层聚类被称为合成聚类或HAC。

层次聚类算法的两种实现思想，一种是初始时将每个待聚类的数据样本视为一个cluster,采用合并的方式，每次合并两个"距离"最近的cluster，直到合并成一个cluster为止(当然可以在达到自己设定想得到的cluster个数时终止迭代)；另一种刚好与第一种相反，初始时将所有的数据样本视为一个cluster,采用分解的方式。

该算法初始时，将每个点作为一个簇，每一步合并两个最接近的簇。即使到最后，对于噪音点或是离群点也往往还是各占一簇的，除非过度合并。

对于这里的“最接近”，有下面三种定义。我在实现是使用了MIN，该方法在合并时，只要依次取当前最近的点对，如果这个点对当前不在一个簇中，将所在的两个簇合并就行：

* 单链(MIN):定义簇的邻近度为不同两个簇的两个最近的点之间的距离。
* 全链(MAX):定义簇的邻近度为不同两个簇的两个最远的点之间的距离。
* 组平均：定义簇的邻近度为取自两个不同簇的所有点对邻近度的平均值。

具体步骤： 
1. 首先我们将每个数据点视为一个单一的簇，然后选择一个测量两个簇之间距离的度量标准。例如我们使用average linkage作为标准，它将两个簇之间的距离定义为第一个簇中的数据点与第二个簇中的数据点之间的平均距离。 
2. 在每次迭代中，我们将两个具有最小average linkage的簇合并成为一个簇。 
3. 重复步骤2知道所有的数据点合并成一个簇，然后选择我们需要多少个簇。

优点：不需要知道有多少个簇 ；对于距离度量标准的选择并不敏感 

缺点：效率低

####  [GMM](https://github.com/stober/gmm) 

用高斯混合模型（GMM）做聚类首先假设数据点是呈高斯分布的，相对应K-Means假设数据点是圆形的，高斯分布（椭圆形）给出了更多的可能性。我们有两个参数来描述簇的形状：均值和标准差。所以这些簇可以采取任何形状的椭圆形，因为在x，y方向上都有标准差。因此，每个高斯分布被分配给单个簇。 

所以要做聚类首先应该找到数据集的均值和标准差，我们将采用一个叫做最大期望(EM)的优化算法。

具体步骤： 
1. 选择簇的数量（与K-Means类似）并随机初始化每个簇的高斯分布参数（均值和方差）。也可以先观察数据给出一个相对精确的均值和方差。 
2. 给定每个簇的高斯分布，计算每个数据点属于每个簇的概率。一个点越靠近高斯分布的中心就越可能属于该簇。 
3. 基于这些概率我们计算高斯分布参数使得数据点的概率最大化，可以使用数据点概率的加权来计算这些新的参数，权重就是数据点属于该簇的概率。 
4. 重复迭代2和3直到在迭代中的变化不大。 

优点：
* GMMs使用均值和标准差，簇可以呈现出椭圆形而不是仅仅限制于圆形。K-Means是GMMs的一个特殊情况，是方差在所有维度上都接近于0时簇就会呈现出圆形。 
* GMMs是使用概率，所有一个数据点可以属于多个簇。例如数据点X可以有百分之20的概率属于A簇，百分之80的概率属于B簇。也就是说GMMs可以支持混合资格。

####  [Affinity Propagation](https://github.com/GGiecold/Concurrent_AP) 

 AP(Affinity Propagation)通常被翻译为近邻传播算法或者亲和力传播算法，是在2007年的Science杂志上提出的一种新的聚类算法。AP算法的基本思想是将全部数据点都当作潜在的聚类中心(称之为exemplar)，然后数据点两两之间连线构成一个网络(相似度矩阵)，再通过网络中各条边的消息(responsibility和availability)传递计算出各样本的聚类中心。


###  www.tfzoo.com

